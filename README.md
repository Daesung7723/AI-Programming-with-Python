
### **ການແນະນຳຫຼັກສູດ**

**ພາບລວມຂອງຫຼັກສູດ:** ຫຼັກສູດນີ້ແມ່ນຫຼັກສູດທີ່ຄົບຖ້ວນ, ອອກແບບມາສຳລັບນັກສຶກສາວິສະວະກຳທີ່ບໍ່ມີພື້ນຖານດ້ານການຂຽນໂປຣແກຣມ AI. ຜ່ານການຝຶກອົບຮົມແບບເຂັ້ມຂຸ້ນເປັນເວລາ 10 ວັນ, ຜູ້ຮຽນຈະມີຄວາມສາມາດໃນການສ້າງໂມເດລ AI ແບບງ່າຍດາຍ ແລະ ແອັບພລິເຄຊັນທີ່ອີງໃສ່ API.

**ເປົ້າໝາຍສູງສຸດ:** ເກີນກວ່າການຮຽນຮູ້ວິທີໃຊ້ເຄື່ອງມືທົ່ວໄປ, ເປົ້າໝາຍຂອງພວກເຮົາແມ່ນເພື່ອສ້າງ "ນັກພັດທະນາ AI" ທີ່ເຂົ້າໃຈຢ່າງເລິກເຊິ່ງກ່ຽວກັບຫຼັກການເຮັດວຽກຂອງໂມເດລ AI ແລະ ມີຄວາມສາມາດໃນການແກ້ໄຂບັນຫາໃນໂລກຄວາມຈິງ.

### **ປັດຊະຍາ ແລະ ເປົ້າໝາຍຂອງຫຼັກສູດ**

* **ປັດຊະຍາການສອນ:** ການຮຽນຮູ້ທີ່ສົມດຸນຜ່ານການผสมสานລະຫວ່າງທິດສະດີ (40%) ແລະ ການປະຕິບັດໂຕຈິງ (60%) ຢ່າງເປັນລະບົບ.  
* **ທິດທາງການຮຽນຮູ້:** ສົ່ງເສີມໃຫ້ນັກຮຽນເຂົ້າໃຈທັງ "ເປັນຫຍັງ (Why)" ແລະ "ແນວໃດ (How)" ຂອງເທັກໂນໂລຢີ, ເພື່ອສ້າງຄວາມສາມາດໃນການຮັບມືກັບສະພາບແວດລ້ອມທາງເທັກໂນໂລຢີທີ່ປ່ຽນແປງ ແລະ ແກ້ໄຂບັນຫາດ້ວຍຕົນເອງ.  
* **ເປົ້າໝາຍສຸດທ້າຍຂອງການຝຶກອົບຮົມ:**  
  * ສ້າງຄວາມສາມາດໃນການນຳໃຊ້ PyTorch ເພື່ອສ້າງ ແລະ ຝຶກສອນໂມເດລ Deep Learning ຫຼັກເຊັ່ນ CNN, RNN ດ້ວຍຕົນເອງ.  
  * ຮັບປະກັນຄວາມສາມາດໃນການນຳໃຊ້ Google Gemini API ເພື່ອເຊື່ອມໂຍງຟັງຊັນ AI ຂັ້ນສູງ ເຊັ່ນ การสร้างข้อความ, การวิเคราะห์ภาพ ເຂົ້າໃນແອັບພລິເຄຊັນ.  
  * ສະສົມປະສົບການໃນການເຮັດວຽກຈິງຜ່ານໂຄງການກຸ່ມ, ຕັ້ງແຕ່ການວາງແຜນ, ການພັດທະນາ, ຈົນເຖິງການນຳສະເໜີແອັບພລິເຄຊັນ AI ທີ່ສາມາດໃຊ້ງານໄດ້ຈິງ.

### **ສະແຕັກເທັກໂນໂລຢີ ແລະ ສະພາບແວດລ້ອມການຮຽນຮູ້**

* **ພາສາຫຼັກ:** Python  
* **ໄລບຣາຣີຫຼັກ:** NumPy, Pandas, Matplotlib, Seaborn, PyTorch  
* **ສະພາບແວດລ້ອມການພັດທະນາ:** Google Colab. ບໍ່ຈຳເປັນຕ້ອງຕິດຕັ້ງສະພາບແວດລ້ອມໃນຄອມພິວເຕີສ່ວນຕົວ, ສາມາດເຂົ້າຮ່ວມການປະຕິບັດໄດ້ໂດຍໃຊ້ພຽງແຕ່ເວັບເບຣາວເຊີ.  
  * ຮອງຮັບ GPU ຟຣີ ເພື່ອຊ່ວຍຫຼຸດເວລາໃນການຝຶກສອນໂມເດລ Deep Learning ທີ່ซับซ้อน ແລະ ເພີ່ມຄວາມຕັ້ງใจໃນການຮຽນ.  
* **API ແລະ ແອັບພລິເຄຊັນ:** Google Gemini API, Streamlit.  
  * **Streamlit:** ເປັນເຄື່ອງມືທີ່ຊ່ວຍສ້າງເວັບແອັບພລິເຄຊັນແບບໂຕ້ຕອບໄດ້ຢ່າງรวดเร็วໂດຍໃຊ້ພຽງແຕ່ລະຫັດ Python, ເຮັດໃຫ້ມີປະສົບການໃນການສ້າງໂມເດລ AI ເພື່ອให้บริการ.

### **ຕາຕະລາງລວມຂອງຫຼັກສູດການຂຽນໂປຣແກຣມ AI 10 ວັນ**

| ມື້ທີ | ພາກເຊົ້າ (09:00-12:00) | ພາກບ່າຍ (13:00-17:00) | ເປົ້າໝາຍການຮຽນຮູ້ຫຼັກ | ການປະຕິບັດໂຕຈິງຫຼັກ |
| :---- | :---- | :---- | :---- | :---- |
| **1** | **ທິດສະດີ:** ພາບລວມຂອງ AI ແລະ ຂັ້ນຕອນການເຮັດວຽກຂອງ Machine Learning | **ປະຕິບັດ:** NumPy, ຫົວໃຈຫຼັກຂອງການຄຳນວນຕົວເລກ | ເຂົ້າໃຈຄວາມສຳພັນລະຫວ່າງ AI, ML, DL ແລະ ຄວາມສຳຄັນຂອງການຄຳນວນແບບເວັກເຕີ. | ການສ້າງ, ຈັດການ NumPy ndarray ແລະ ການຄຳນວນແບບເວັກເຕີ. |
| **2** | **ປະຕິບັດ:** Pandas, ສິລະປະການຈັດການຂໍ້ມູນ | **ປະຕິບັດ:** ການສະແດງພາບຂໍ້ມູນ, Matplotlib & Seaborn | ສ້າງຄວາມສາມາດໃນການກັ່ນຕອງຂໍ້ມູນ ແລະ ການວິເຄາະຂໍ້ມູນເບື້ອງຕົ້ນ (EDA). | ການທຳຄວາມສະອາດຂໍ້ມູນ ແລະ ການສະແດງພາບຂໍ້ມູນໂດຍໃຊ້ຂໍ້ມູນຈິງ. |
| **3** | **ທິດສະດີ/ປະຕິບັດ:** Tensor, ຫົວໃຈຂອງ PyTorch | **ທິດສະດີ/ປະຕິບັດ:** torch.autograd, ສິ່ງມະຫັດສະຈັນຂອງການ 微分 ອັດຕະໂນມັດ | ເຂົ້າໃຈການຄຳນວນ Tensor ໂດຍໃຊ້ GPU ແລະ ຫຼັກການຝຶກສອນ Deep Learning (Backpropagation). | ການສ້າງໂມເດລ Linear Regression ດ້ວຍຕົນເອງ ແລະ ການຄຳນວນ Gradient. |
| **4** | **ທິດສະດີ/ປະຕິບັດ:** nn.Module, ໂຄງສ້າງຂອງໂມເດລ | **ທິດສະດີ/ປະຕິບັດ:** Optimizer ແລະ DataLoader, ເຄື່ອງຈັກຂອງການຮຽນຮູ້ | ສ້າງລະບົບການຮຽນຮູ້ ແລະ ສ້າງເຄືອຂ່າຍປະສາດໂດຍໃຊ້ PyTorch ໃຫ້ສົມບູນ. | ການສ້າງ Loop ການຮຽນຮູ້ໂດຍໃຊ້ nn.Module ແລະ DataLoader. |
| **5** | **ທິດສະດີ:** CNN, ດວງຕາທີ່ເບິ່ງເຫັນຮູບພາບ | **ປະຕິບັດ:** ສ້າງຕົວຈຳແນກຮູບພາບ CIFAR-10 | ເຂົ້າໃຈໂຄງສ້າງ ແລະ ຫຼັກການເຮັດວຽກຂອງ CNN (Convolution, Pooling). | ການສ້າງໂມເດລ CNN ໂດຍໃຊ້ torchvision ແລະ nn.Conv2d. |
| **6** | **ທິດສະດີ:** RNN/LSTM, ການອ່ານກະແສເວລາ | **ປະຕິບັດ:** ຈຳແນກສັນຊາດດ້ວຍ RNN ລະດັບຕົວອັກສອນ | ເຂົ້າໃຈການປະມວນຜົນຂໍ້ມູນລຳດັບ ແລະ ວິທີແກ້ໄຂບັນຫາ Long-Term Dependency ຂອງ RNN/LSTM. | ການສ້າງໂມເດລ RNN ທີ່ປະມວນຜົນຂໍ້ມູນຂໍ້ຄວາມທີ່ມີຄວາມຍາວປ່ຽນແປງໄດ້. |
| **7** | **ທິດສະດີ:** Transformer & Attention, ຢືນຢູ່ເທິງບ່າຂອງຍັກໃຫຍ່ | **ປະຕິບັດ:** ການເລີ່ມຕົ້ນກັບ Gemini API | ເຂົ້າໃຈແນວຄິດຂອງ Transformer ແລະ ກົນໄກ Self-Attention. | ການສ້າງຂໍ້ຄວາມ ແລະ ການປະມວນຜົນຂໍ້ມູນແບບ Multimodal ໂດຍໃຊ້ Gemini API. |
| **8** | **ປະຕິບັດ:** ການສ້າງ Web UI ດ້ວຍ Python, Streamlit | **ປະຕິບັດ:** ສ້າງ Chatbot ໂຕ້ຕອບທີ່ເຊື່ອມຕໍ່ກັບ Gemini API | ສ້າງຄວາມສາມາດໃນການພັດທະນາເວັບແອັບພລິເຄຊັນເພື່ອໃຫ້ບໍລິການໂມເດລ AI. | ການສ້າງ Chatbot ໂຕ້ຕອບໂດຍໃຊ້ st.session\_state. |
| **9** | **ໂຄງການ:** ວາງແຜນໂຄງການສຸດທ້າຍ ແລະ Sprint ການພັດທະນາ 1 | **ໂຄງການ:** Sprint ການພັດທະນາ 1 (ຕໍ່) | ການວາງແຜນ ແລະ ພັດທະນາຟັງຊັນຫຼັກຂອງແອັບພລິເຄຊັນ AI ຜ່ານການຮ່ວມມືໃນກຸ່ມ. | ການອອກແບບສະຖາປັດຕະຍະກຳຂອງໂຄງການ, ແບ່ງໜ້າທີ່ ແລະ ເລີ່ມການພັດທະນາ. |
| **10** | **ໂຄງການ:** Sprint ການພັດທະນາ 2 ແລະ ການກະກຽມນຳສະເໜີ | **ໂຄງການ:** ນຳສະເໜີໂຄງການສຸດທ້າຍ ແລະ ສຳເລັດຫຼັກສູດ | ສ້າງໂຄງການໃຫ້ສຳເລັດ, ນຳສະເໜີ, ແລະ ຮັບຄຳຕິຊົມເພື່ອເສີມສ້າງຄວາມສາມາດ. | ນຳສະເໜີ ແລະ ສາທິດຜົນງານຂອງກຸ່ມ, ສະຫຼຸບຫຼັກສູດ. |

---

### **ແຜນການສອນລະອຽດແຕ່ລະມື້**

#### **ມື້ທີ 1: ຮາກຖານຂອງ AI \- ສູ່ການເປັນມາສເຕີ Python ສໍາລັບວິທະຍາສາດຂໍ້ມູນ**

* **ພາກເຊົ້າ (09:00-12:00) \- ທິດສະດີ: ພາບລວມຂອງ AI ແລະ ຂັ້ນຕອນການເຮັດວຽກຂອງ Machine Learning**  
  * **ເປົ້າໝາຍ:** ສ້າງແຮງຈູງໃຈໃນການຮຽນຮູ້ ແລະ ຈັດລະບົບຄວາມຮູ້ໂດຍການກຳນົດຕຳແໜ່ງຂອງເທັກໂນໂລຢີໃນຂະບວນການພັດທະນາ AI ທັງໝົດ.  
  * **ເນື້ອໃນ:**  
    * **ຄວາມສຳພັນລະຫວ່າງ AI, ML, DL:** ນິຍາມ ແລະ ອະທິບາຍຄວາມສຳພັນແບບ AI \> ML \> DL.  
    * **ປະເພດການຮຽນຮູ້:** ແນະນຳແນວຄິດ ແລະ ຕົວຢ່າງຂອງ Supervised, Unsupervised, and Reinforcement Learning.  
    * **ວົງຈອນຊີວິດຂອງໂຄງການ ML:** ອະທິບາຍ 6 ຂັ້ນຕອນການເຮັດວຽກ, ຕັ້ງແຕ່ການກຳນົດບັນຫາຈົນເຖິງການນຳໄປໃຊ້.  
* **ພາກບ່າຍ (13:00-17:00) \- ປະຕິບັດ: NumPy, ຫົວໃຈຫຼັກຂອງການຄຳນວນຕົວເລກ**  
  * **ເປົ້າໝາຍ:** ສ້າງຄວາມຊຳນານໃນ NumPy ເພື່ອຈັດການຂໍ້ມູນຕົວເລກຂະໜາດໃຫຍ່ຢ່າງรวดเร็ว ແລະ ມີประสิทธิภาพ, ເຊິ່ງເປັນພື້ນຖານຂອງ Deep Learning.  
  * **ເນື້ອໃນ:**  
    * **ຄວາມຈຳເປັນຂອງ NumPy:** ສະແດງໃຫ້ເຫັນປະສິດທິພາບທີ່ເໜືອກວ່າຂອງ 'Vectorized Operation' ເມື່ອທຽບກັບ Python List.  
    * **ປະຕິບັດຟັງຊັນຫຼັກ:**  
      * ການສ້າງ ndarray (np.array, np.zeros) ແລະ ການຈັດການ (reshape).  
      * ການ Indexing ແລະ Slicing.  
      * ການຄຳນວນແບບເວັກເຕີ (+, \-, \*, /) ແລະ ການເຂົ້າໃຈກົດຂອງ Broadcasting.  
      * ການສະກັດຂໍ້ມູນຕາມເງື່ອນໄຂໂດຍໃຊ້ Boolean Masking.

#### **ມື້ທີ 2: ການຈັດການຂໍ້ມູນ \- Pandas ແລະ Matplotlib**

* **ພາກເຊົ້າ (09:00-12:00) \- ປະຕິບັດ: Pandas, ສິລະປະການຈັດການຂໍ້ມູນ**  
  * **ເປົ້າໝາຍ:** ສ້າງຄວາມຊຳນານໃນໄລບຣາຣີ Pandas, ເຊິ່ງຈຳເປັນສຳລັບການໂຫຼດ, ກັ່ນຕອງ, ແລະ ປະມວນຜົນຂໍ້ມູນທີ່ມີໂຄງສ້າງ.  
  * **ເນື້ອໃນ:**  
    * **ໂຄງສ້າງຂໍ້ມູນຫຼັກ:** ການສ້າງ ແລະ ຈັດການ Series (1 ມິຕິ) ແລະ DataFrame (2 ມິຕິ).  
    * **ການນຳເຂົ້າ/ສົ່ງອອກຂໍ້ມູນ:** pd.read\_csv, pd.read\_excel.  
    * **ການກັ່ນຕອງຂໍ້ມູນ:** ການຈັດການຄ່າທີ່ຂາດຫາຍ (isnull, dropna, fillna), ການປ່ຽນປະເພດຂໍ້ມູນ (to\_datetime).  
    * **ການເລືອກ/ປະມວນຜົນຂໍ້ມູນ:** ການເລືອກທີ່ຊັດເຈນໂດຍໃຊ້ loc/iloc, ການກອງຕາມເງື່ອນໄຂ, ການຈັດກຸ່ມ ແລະ ການລວບລວມຂໍ້ມູນໂດຍໃຊ້ groupby (sum, mean, agg).  
* **ພາກບ່າຍ (13:00-17:00) \- ປະຕິບັດ: ການສະແດງພາບຂໍ້ມູນ, Matplotlib & Seaborn**  
  * **ເປົ້າໝາຍ:** ສ້າງຄວາມສາມາດໃນການສະແດງພາບເພື່ອຄົ້ນພົບຮູບແບບ ແລະ ຄວາມເຂົ້າໃຈທີ່ซ่อนอยู่ในຂໍ້ມູນ.  
  * **ເນື້ອໃນ:**  
    * **ພື້ນຖານ Matplotlib:** ການສ້າງກຣາຟພື້ນຖານ ເຊັ່ນ: ກຣາຟເສັ້ນ (plot), ກຣາຟແທ່ງ (bar), ແຜນວາດກະຈາຍ (scatter), ຮິສໂຕແກຣມ (hist).  
    * **ການໃຊ້ Seaborn:** ສ້າງກຣາຟທີ່ສວຍງາມ ແລະ ມີຄວາມໝາຍທາງສະຖິຕິດ້ວຍລະຫັດທີ່ໜ້ອຍລົງ.  
    * **ການວິເຄາະຂໍ້ມູນເບື້ອງຕົ້ນ (EDA):** ການສະແດງພາບການກະຈາຍຂອງຕົວແປ (ຮິສໂຕແກຣມ, box plot) ແລະ ຄວາມສຳພັນລະຫວ່າງຕົວແປ (ແຜນວາດກະຈາຍ, heatmap) ເພື່ອສະກັດຄວາມເຂົ້າໃຈ.

#### **ມື້ທີ 3: ເປີດປະຕູສູ່ Deep Learning \- ພື້ນຖານ PyTorch**

* **ພາກເຊົ້າ (09:00-12:00) \- ທິດສະດີ ແລະ ປະຕິບັດ: Tensor, ຫົວໃຈຂອງ PyTorch**  
  * **ເປົ້າໝາຍ:** ຮຽນຮູ້ແນວຄິດ ແລະ ວິທີໃຊ້ Tensor, ເຊິ່ງເປັນໂຄງສ້າງຂໍ້ມູນຫຼັກຂອງ PyTorch ທີ່ສາມາດເລັ່ງການຄຳນວນດ້ວຍ GPU.  
  * **ເນື້ອໃນ:**  
    * **ແນວຄິດ Tensor:** ເປັນອາເຣຫຼາຍມິຕິທີ່ຄ້າຍຄືกับ NumPy ndarray, ແຕ່ຮອງຮັບການຄຳນວນເທິງ GPU.  
    * **ການສ້າງ/ຈັດການ Tensor:** torch.tensor, torch.randn, indexing, slicing, view(reshape).  
    * **ການໃຊ້ GPU:** ທົດລອງເລັ່ງການຄຳນວນໂດຍການຍ້າຍ Tensor ໄປທີ່ GPU ດ້ວຍเมธอด .to('cuda').  
* **ພາກບ່າຍ (13:00-17:00) \- ທິດສະດີ ແລະ ປະຕິບັດ: torch.autograd, ສິ່ງມະຫັດສະຈັນຂອງການ 微分 ອັດຕະໂນມັດ**  
  * **ເປົ້າໝາຍ:** ເຂົ້າໃຈຫຼັກການຫຼັກຂອງການຮຽນຮູ້ Deep Learning: Backpropagation ແລະ Autograd.  
  * **ເນື້ອໃນ:**  
    * **ຫຼັກການຮຽນຮູ້ Deep Learning:**  
      * **Gradient Descent:** ຂະບວນການອັບເດດພາຣາມິເຕີໄປໃນທິດທາງທີ່ເຮັດໃຫ້ Loss ຫຼຸດລົງ.  
      * **Backpropagation:** ອັນກໍຣິທຶມທີ່ຄຳນວນ Gradient ຂອງແຕ່ລະພາຣາມິເຕີຢ່າງມີປະສິດທິພາບ.  
    * **ປະຕິບັດ autograd:**  
      * requires\_grad=True: ເປີດໃຊ້ການຕິດຕາມການຄຳນວນຂອງ Tensor ເພື່ອຄຳນວນ Gradient.  
      * loss.backward(): ທຳການ Backpropagation ອັດຕະໂນມັດ ແລະ ເກັບ Gradient ໄວ້ໃນ .grad.  
    * **ວຽກບ້ານ:** ສ້າງໂມເດລ Linear Regression (y=w×x+b) ດ້ວຍຕົນເອງ, ຈາກນັ້ນเรียก loss.backward() ແລະ ໃຊ້ w.grad, b.grad ເພື່ອອັບເດດພາຣາມິເຕີ ແລະ ສັງເກດເບິ່ງການຫຼຸດລົງຂອງ Loss.

#### **ມື້ທີ 4: ການສ້າງເຄືອຂ່າຍປະສາດທີ່ແທ້ຈິງ \- ການສ້າງ ແລະ ການຝຶກສອນໂມເດລ**

* **ພາກເຊົ້າ (09:00-12:00) \- ທິດສະດີ ແລະ ປະຕິບັດ: nn.Module ແລະ Loss Function**  
  * **ເປົ້າໝາຍ:** ສ້າງໂມເດລເຄືອຂ່າຍປະສາດທີ່ນຳກັບມາໃຊ້ໃໝ່ໄດ້ໂດຍໃຊ້ API ລະດັບສູງຂອງ PyTorch.  
  * **ເນື້ອໃນ:**  
    * **torch.nn.Module:** ເປັນຄລາສແມ່ຂອງໂມເດລເຄືອຂ່າຍປະສາດທັງໝົດ.  
      * \_\_init\_\_(self): ນິຍາມຊັ້ນຕ່າງໆ (nn.Linear) ທີ່ຈຳເປັນໃນໂມເດລ.  
      * forward(self, x): ນິຍາມຂັ້ນຕອນການຄຳນວນຂອງຂໍ້ມູນຂາເຂົ້າ.  
    * **Loss Function:** ຮຽນຮູ້ການໃຊ້ Loss Function ທີ່ສ້າງໄວ້ລ່ວງໜ້າ ເຊັ່ນ nn.MSELoss (Regression), nn.CrossEntropyLoss (Classification).  
    * **ວຽກບ້ານ:** ຂຽນໂມເດລ Linear Regression ຈາກມື້ທີ 3 ໃໝ່ໃນຮູບແບບຂອງຄລາສໂດຍໃຊ້ nn.Module ແລະ nn.Linear.  
* **ພາກບ່າຍ (13:00-17:00) \- ທິດສະດີ ແລະ ປະຕິບັດ: Optimizer ແລະ DataLoader**  
  * **ເປົ້າໝາຍ:** ເຂົ້າໃຈບົດບາດ ແລະ ການນຳໃຊ້ Optimizer ແລະ DataLoader, ເຊິ່ງຊ່ວຍໃຫ້ຂະບວນການຮຽນຮູ້เป็นไปโดยอัตโนมัติ.  
  * **ເນື້ອໃນ:**  
    * **Optimizer:** ການອັບເດດພາຣາມິເຕີອັດຕະໂນມັດໂດຍໃຊ້ແພັກເກດ torch.optim (optim.SGD, optim.Adam).  
    * **Standard Training Loop (3 ຂັ້ນຕອນ):**  
      1. optimizer.zero\_grad(): ລ້າງຄ່າ Gradient.  
      2. loss.backward(): ຄຳນວນ Gradient ດ້ວຍ Backpropagation.  
      3. optimizer.step(): ອັບເດດພາຣາມິເຕີ.  
    * **Dataset & DataLoader:** ຈັດການຂໍ້ມູນຂະໜາດໃຫຍ່ຢ່າງມີປະສິດທິພາບ.  
      * **Dataset:** ນິຍາມໂລຈິກໃນການສົ່ງຄືນຂໍ້ມູນ ແລະ ປ້າຍກຳກັບເທື່ອລະອັນ.  
      * **DataLoader:** ຮັບ Dataset ມາຈັດເປັນກຸ່ມ (Batch) ແລະ ໃຫ້ຟັງຊັນຕ່າງໆ ເຊັ່ນ ການสับเปลี่ยน.  
    * **ວຽກບ້ານ:** ຮວມ Dataset, DataLoader, nn.Module, Optimizer ເຂົ້າກັນເພື່ອສ້າງ Loop ການຮຽນຮູ້ທີ່ສົມບູນ.

#### **ມື້ທີ 5: ດວງຕາທີ່ເບິ່ງເຫັນຮູບພາບ \- CNN (ເຄືອຂ່າຍປະສາດແບບຄອນໂວລູຊັນ)**

* **ພາກເຊົ້າ (09:00-12:00) \- ທິດສະດີ: ຄວາມເຂົ້າໃຈທາງສາຍຕາຂອງ CNN**  
  * **ເປົ້າໝາຍ:** ເຂົ້າໃຈໂຄງສ້າງ ແລະ ຫຼັກການເຮັດວຽກຂອງ CNN, ເຊິ່ງຊ່ຽວຊານໃນການປະມວນຜົນຮູບພາບ.  
  * **ເນື້ອໃນ:**  
    * **Convolution Operation:** Kernel (Filter) ສະແກນຮູບພາບເພື່ອສະກັດຄຸນລັກສະນະສະເພາະຈຸດ (ເສັ້ນ, ພື້ນຜິວ) ແລະ ສ້າງ Feature Map.  
    * **ພາຣາມິເຕີຫຼັກ:** ເຂົ້າໃຈ Channel, Padding, Stride.  
    * **Pooling Layer:** ຫຼຸດຂະໜາດຂອງ Feature Map (Downsampling) ເພື່ອຫຼຸດປະລິມານການຄຳນວນ ແລະ ເຮັດໃຫ້ໂມເດລບໍ່ອ່ອນໄຫວຕໍ່ການປ່ຽນແປງຕຳແໜ່ງ.  
    * **Feature Hierarchy:** ເຂົ້າໃຈໂຄງສ້າງທີ່ຮຽນຮູ້ຈາກຄຸນລັກສະນະລະດັບຕ່ຳ (ເສັ້ນ, ຂອບ) ໄປສູ່ຄຸນລັກສະນະລະດັບສູງ (ຕາ, ດັງ, ປາກ).  
* **ພາກບ່າຍ (13:00-17:00) \- ປະຕິບັດ: ສ້າງຕົວຈຳແນກຮູບພາບ CIFAR-10**  
  * **ເປົ້າໝາຍ:** ລະດົມຄວາມຮູ້ PyTorch ທັງໝົດເພື່ອສ້າງຕົວຈຳແນກຮູບພາບຈິງ.  
  * **ເນື້ອໃນ:**  
    * **ການກະກຽມຂໍ້ມູນ:** ດາວໂຫຼດຊຸດຂໍ້ມູນ CIFAR-10 ດ້ວຍ torchvision, ທຳການ tiềnประมวลผล ເຊັ່ນ Normalization ແລະ ສ້າງ DataLoader.  
    * **ການນິຍາມໂມເດລ CNN:** ອອກແບບໂຄງສ້າງ CNN ໂດຍການผสมผสาน nn.Conv2d ແລະ nn.MaxPool2d ໃນ nn.Module.  
    * **ການຝຶກສອນ ແລະ ປະເມີນຜົນ:** ຝຶກສອນໂມເດລໂດຍໃຊ້ nn.CrossEntropyLoss ແລະ optim.SGD, ແລະ ປະເມີນຄວາມแม่นยำด้วยชุดข้อมูลทดสอบ.

#### **ມື້ທີ 6: ການອ່ານກະແສເວລາ \- RNN (ເຄືອຂ່າຍປະສາດແບບຮອບວຽນ)**

* **ພາກເຊົ້າ (09:00-12:00) \- ທິດສະດີ: ຫຼັກການຂອງຂໍ້ມູນລຳດັບ ແລະ RNN/LSTM**  
  * **ເປົ້າໝາຍ:** ເຂົ້າໃຈຫຼັກການຂອງ RNN ແລະ LSTM, ເຊິ່ງເປັນໂມເດລສຳລັບການປະມວນຜົນຂໍ້ມູນລຳດັບ ເຊັ່ນ ຂໍ້ຄວາມ, ຂໍ້ມູນอนุกรมเวลา.  
  * **ເນື້ອໃນ:**  
    * **RNN (Recurrent Neural Network):** ປະມວນຜົນຂໍ້ມູນປັດຈຸບັນພ້ອມກັບຂໍ້ມູນຈາກຂັ້ນຕອນເວລາກ່ອນໜ້າ (Hidden State) ຜ່ານໂຄງສ້າງແບບຮອບວຽນ.  
    * **ບັນຫາ Long-Term Dependency:** ຂໍ້ຈຳກັດຂອງ RNN ທີ່ຂໍ້ມູນໃນອະດີດຈະຫາຍໄປເມື່ອລຳດັບຍາວຂຶ້ນ.  
    * **LSTM (Long Short-Term Memory):** ໂຄງສ້າງ RNN ຂັ້ນສູງເພື່ອແກ້ໄຂບັນຫານี้.  
      * **Cell State:** ເປັນ 'ສາຍພານຄວາມຈຳ' ທີ່ສາມາດເກັບຂໍ້ມູນໄດ້ໃນໄລຍະຍາວ.  
      * **Gates:** ຄວບຄຸມຂໍ້ມູນໃນ Cell State ຢ່າງລະອຽດດ້ວຍ 3 ເກດ: Forget, Input, Output.  
* **ພາກບ່າຍ (13:00-17:00) \- ປະຕິບັດ: ຈຳແນກສັນຊາດຂອງຊື່ດ້ວຍ RNN ລະດັບຕົວອັກສອນ**  
  * **ເປົ້າໝາຍ:** ປະຕິບັດການສ້າງໂມເດລ RNN ທີ່ປະມວນຜົນຂໍ້ມູນລຳດັບທີ່ມີຄວາມຍາວປ່ຽນແປງໄດ້.  
  * **ເນື້ອໃນ:**  
    * **ການ tiềnประมวลผลข้อมูล:** ແປງຂໍ້ຄວາມ (ຕົວອັກສອນ) ໃຫ້ເປັນດັດຊະນີຕົວເລກ ແລະ One-hot Vector Tensor.  
    * **ການນິຍາມໂມເດລ RNN:** ສ້າງໂມເດລ RNN ລະດັບຕົວອັກສອນໂດຍໃຊ້ nn.RNN ຫຼື nn.LSTM ໃນ nn.Module.  
    * **ການຝຶກສອນ ແລະ ຄາດເດົາ:** ຫຼັງຈາກຝຶກສອນໂມເດລ, ປ້ອນຊື່ໃໝ່ເພື່ອຄາດເດົາສັນຊາດ ແລະ ສັງເກດເບິ່ງຂະບວນການຮຽນຮູ້ຮູບແບບລຳດັບ.

#### **ມື້ທີ 7: ຢືນຢູ່ເທິງບ່າຂອງຍັກໃຫຍ່ \- LLM ແລະ Gemini API**

* **ພາກເຊົ້າ (09:00-12:00) \- ທິດສະດີ: Transformer ແລະ Attention Mechanism**  
  * **ເປົ້າໝາຍ:** ເຂົ້າໃຈຫຼັກການຫຼັກຂອງສະຖາປັດຕະຍະກຳ Transformer, ເຊິ່ງເປັນພື້ນຖານຂອງ NLP ສະໄໝໃໝ່.  
  * **ເນື້ອໃນ:**  
    * **ທີ່ມາຂອງ Transformer:** ເກີດຂຶ້ນເພື່ອແກ້ໄຂຂໍ້ຈຳກັດຂອງ RNN (ການປະມວນຜົນແບບຂະໜານບໍ່ໄດ້, Long-Term Dependency).  
    * **Self-Attention:** ກົນໄກຫຼັກທີ່ຄຳນວນຄວາມກ່ຽວຂ້ອງລະຫວ່າງຄຳສັບທັງໝົດໃນປະໂຫຍກພ້ອມກັນເພື່ອເຂົ້າໃຈບໍລິບົດ. ເຮັດວຽກຜ່ານ Query, Key, Value vectors.  
    * **ໂຄງສ້າງ Transformer:** ໂຄງສ້າງ Encoder-Decoder, ການຮຽນຮູ້ບໍລິບົດຫຼາຍມຸມມອງຜ່ານ Multi-head Attention.  
* **ພາກບ່າຍ (13:00-17:00) \- ປະຕິບັດ: ການເລີ່ມຕົ້ນກັບ Gemini API**  
  * **ເປົ້າໝາຍ:** ທົດລອງການປ່ຽນແປງກະบวนทัศน์ໂດຍການນຳໃຊ້ໂມເດລຂະໜາດໃຫຍ່ທີ່ຝຶກສອນໄວ້ລ່ວງໜ້າ (LLM) ຜ່ານ API.  
  * **ເນື້ອໃນ:**  
    * **ການຂໍ ແລະ ຕັ້ງຄ່າ API Key:** ຂໍ API Key ຈາກ Google AI Studio ແລະ ຕັ້ງຄ່າສະພາບແວດລ້ອມ.  
    * **ການໃຊ້ Python SDK:** ຕິດຕັ້ງ pip install google-generativeai ແລະ ຕັ້ງຄ່າພື້ນຖານ.  
    * **ການສ້າງຂໍ້ຄວາມ:** ທົດລອງສ້າງຂໍ້ຄວາມພື້ນຖານດ້ວຍຟັງຊັນ model.generate\_content() ແລະ ສຳຫຼວດ Prompt Engineering.  
    * **ການປະມວນຜົນ Multimodal Input:** ໃຊ້ຂໍ້ຄວາມ ແລະ ຮູບພາບເປັນ Input ພ້ອມກັນເພື່ອສ້າງຟັງຊັນ AI ຂັ້ນສູງ ເຊັ່ນ ການອະທິບາຍຮູບພາບ, ການຖາມ-ຕອບ.

#### **ມື້ທີ 8: ປ່ຽນ AI ໃຫ້ເປັນບໍລິການ \- ການສ້າງແອັບພລິເຄຊັນດ້ວຍ Streamlit**

* **ພາກເຊົ້າ (09:00-12:00) \- ປະຕິບັດ: ການສ້າງ Web UI ດ້ວຍ Python, Streamlit**  
  * **ເປົ້າໝາຍ:** ຮຽນຮູ້ເທັກນິກການສ້າງເວັບແອັບພລິເຄຊັນເພື່ອໃຫ້ບໍລິການໂມເດລ AI ໂດຍໃຊ້ພຽງ Python, ໂດຍບໍ່ຕ້ອງໃຊ້ເທັກໂນໂລຢີເວັບທີ່ซับซ้อน.  
  * **ເນື້ອໃນ:**  
    * **ປັດຊະຍາຂອງ Streamlit:** ເຟຣມເວີກພັດທະນາເວັບແອັບທີ່รวดเร็ว ແລະ ງ່າຍດາຍສຳລັບນັກວິທະຍາສາດຂໍ້ມູນ.  
    * **Widget ພື້ນຖານ:** ຮຽນຮູ້ການໃຊ້ UI Components ເຊັ່ນ st.title, st.write, st.button, st.text\_input.  
    * **Layout:** ການຈັດໜ້າຈໍໂດຍໃຊ້ st.sidebar, st.columns.  
    * **State Management:** ການຮັກສາຂໍ້ມູນ (ເຊັ່ນ ປະຫວັດການສົນທະນາ) ລະຫວ່າງການໂຕ້ຕອບໂດຍໃຊ້ st.session\_state.  
* **ພາກບ່າຍ (13:00-17:00) \- ປະຕິບັດ: ສ້າງ Chatbot ໂຕ້ຕອບທີ່ເຊື່ອມຕໍ່ກັບ Gemini API ແລະ Streamlit**  
  * **ເປົ້າໝາຍ:** ລະດົມເທັກນິກທີ່ຮຽນມາທັງໝົດເພື່ອສ້າງ Chatbot ໂຕ້ຕອບທີ່ໃຊ້ງານໄດ້ຈິງ.  
  * **ເນື້ອໃນ:**  
    * **ການສ້າງ UI ຂອງ Chatbot:** ສ້າງ Interface ການສົນທະນາໂດຍໃຊ້ st.chat\_message ແລະ st.chat\_input.  
    * **ການຈັດການປະຫວັດການສົນທະນາ:** ເກັບເນື້ອໃນການສົນທະນາເປັນລາຍການໃນ st.session\_state ແລະ ສະແດງຜົນເທິງໜ້າຈໍ.  
    * **ໂລຈິກການເຊື່ອມຕໍ່ API:** ເມື່ອຜູ້ໃຊ້ປ້ອນຂໍ້ມູນ, ສົ່ງປະຫວັດການສົນທະນາທັງໝົດໄປທີ່ Gemini API ເພື່ອຮັບຄຳຕອບທີ່ເໝາະສົມกับบริบท, ແລ້ວເກັບຄຳຕອບນັ້ນໄວ້ໃນ st.session\_state ແລະ ສະແດງຜົນເທິງໜ້າຈໍ.

#### **ມື້ທີ 9-10: ດ່ານສຸດທ້າຍ \- ໂຄງການກຸ່ມພັດທະນາແອັບພລິເຄຊັນ AI**

* **ພາບລວມ:** ປະສົບການย่อส่วนຂອງຂະບວນການວິສະວະກຳซอฟต์แวร์ທັງໝົດ, ຕັ້ງແຕ່ 'ການກຳນົດບັນຫາ, ການເລືອກເທັກໂນໂລຢີ, ການຮ່ວມມື, ຈົນເຖິງການນຳສະເໜີ'.  
* **ມື້ທີ 9 ພາກເຊົ້າ \- ການວາງແຜນໂຄງການ**  
  * **ເປົ້າໝາຍ:** 'ສ້າງຕົ້ນແບບຂອງບໍລິການ AI ທີ່ໃຊ້ງານໄດ້'.  
  * **Brainstorming:** ສະເໜີ ແລະ thảo luậnໄອເດຍຢ່າງອິດສະຫຼະໂດຍອີງໃສ່ເນື້ອຫາທີ່ຮຽນມາ (PyTorch, Gemini API, Multimodal).  
  * **ການຈັດກຸ່ມ ແລະ ເລືອກໄອເດຍ:** ຈັດກຸ່ມຕາມຄວາມສົນໃຈ ແລະ ຕັດສິນໃຈເລືອກໄອເດຍສຸດທ້າຍ.  
* **ມື້ທີ 9 ພາກບ່າຍ \- Sprint ການພັດທະນາ 1**  
  * **ການອອກແບບ ແລະ ແບ່ງໜ້າທີ່:** ອອກແບບສະຖາປັດຕະຍະກຳຂອງລະບົບ ແລະ ແບ່ງໜ້າທີ່ຮັບຜິດຊອບໃນກຸ່ມ.  
  * **ການພັດທະນາຟັງຊັນຫຼັກ:** ເລີ່ມການພັດທະນາຕາມໜ້າທີ່, ໂດຍມີການໃຫ້ຄຳປຶກສາທາງເທັກນິກຈາກຄູສອນ ແລະ ຜູ້ຊ່ວຍ.  
* **ມື້ທີ 10 ພາກເຊົ້າ \- Sprint ການພັດທະນາ 2 ແລະ ການກະກຽມນຳສະເໜີ**  
  * **ການເຊື່ອມໂຍງ ແລະ Debugging:** ເຊື່ອມໂຍງຟັງຊັນທີ່ພັດທະນາແລ້ວ, ທົດສອບສຸດທ້າຍ, ແລະ ແກ້ໄຂຂໍ້ຜິດພາດ.  
  * **ການກະກຽມນຳສະເໜີ:** ກະກຽມເອກະສານນຳສະເໜີທີ່ປະກອບດ້ວຍຈຸດປະສົງ, ເທັກໂນໂລຢີທີ່ໃຊ້, ຂັ້ນຕອນການສ້າງ, ແລະ ສະຖານະການສາທິດ.  
* **ມື້ທີ 10 ພາກບ່າຍ \- ນຳສະເໜີໂຄງການສຸດທ້າຍ ແລະ ສຳເລັດຫຼັກສູດ**  
  * **ການນຳສະເໜີສຸດທ້າຍ:** ນຳສະເໜີ ແລະ ສາທິດຜົນງານຂອງແຕ່ລະກຸ່ມ.  
  * **ຄຳຕິຊົມ ແລະ ສະຫຼຸບ:** ຮັບຄຳຕິຊົມຈາກໝູ່ເພື່ອນ ແລະ ຄູສອນ, ສະຫຼຸບພາບລວມຂອງຫຼັກສູດ.  
  * **ການສຳເລັດຫຼັກສູດ:** ມອບໃບຢັ້ງຢືນໃຫ້ແກ່ຜູ້ທີ່ຈົບຫຼັກສູດ ແລະ ປິດການຝຶກອົບຮົມ.

## **과정 소개**

* **과정 개요:** AI 프로그래밍을 처음 접하는 공학도 대상, 10일간의 집중 교육을 통해 간단한 AI 모델 및 API 기반 애플리케이션 제작 역량을 갖추도록 설계된 종합 커리큘럼.  
* **궁극적 목표:** 단순 도구 사용법 학습을 넘어, AI 모델의 작동 원리에 대한 깊이 있는 이해와 실제 문제 해결 능력을 겸비한 'AI 개발자' 양성.

### **과정 철학 및 목표**

* **교육 철학:** 이론(40%)과 실습(60%)의 유기적인 결합을 통한 균형 있는 학습.  
* **학습 방향:** 기술의 '왜(Why)'와 '어떻게(How)'를 모두 체득하여, 변화하는 기술 환경에 능동적으로 대처하고 스스로 문제를 해결하는 능력 배양.  
* **최종 교육 목표:**  
  1. PyTorch 프레임워크를 활용하여 CNN, RNN 등 핵심 딥러닝 모델을 직접 구현하고 학습시키는 능력 배양.  
  2. Google Gemini API를 활용하여 텍스트 생성, 이미지 분석 등 고도화된 AI 기능을 애플리케이션에 통합하는 능력 확보.  
  3. 팀 프로젝트를 통해 실제 작동하는 AI 기반 애플리케이션을 기획부터 개발, 발표까지 완료하는 실무 경험 습득.

### **기술 스택 및 학습 환경**

* **주요 언어:** Python  
* **핵심 라이브러리:** NumPy, Pandas, Matplotlib, Seaborn, PyTorch  
* **개발 환경:** Google Colab.  
  * 별도의 로컬 환경 설정 불필요, 웹 브라우저만으로 실습 참여.  
  * 무료 GPU 지원을 통해 복잡한 딥러닝 모델 학습 시간 단축 및 학습 집중도 향상.  
* **API 및 애플리케이션:** Google Gemini API, Streamlit.  
  * Streamlit: Python 코드만으로 빠르게 인터랙티브 웹 애플리케이션을 제작, AI 모델을 서비스로 만드는 경험 제공.

### **10일 AI 프로그래밍 교육과정 전체 일정표**

| 일차 | 오전 세션 (09:00-12:00) | 오후 세션 (13:00-18:00) | 핵심 학습 목표 | 주요 실습 |
| :---- | :---- | :---- | :---- | :---- |
| **1일차** | **이론:** AI의 큰 그림과 머신러닝 워크플로우 | **실습:** 수치 연산의 핵심, NumPy | AI, ML, DL의 관계 이해 및 벡터화 연산의 중요성 체득 | NumPy ndarray 생성, 조작 및 벡터화 연산 실습 |
| **2일차** | **실습:** 데이터 조작의 연금술, Pandas | **실습:** 데이터 시각화, Matplotlib & Seaborn | 데이터 정제 및 탐색적 데이터 분석(EDA) 능력 배양 | 실제 데이터를 활용한 데이터 클리닝 및 시각화 |
| **3일차** | **이론/실습:** PyTorch의 심장, 텐서(Tensor) | **이론/실습:** 자동 미분의 마법, torch.autograd | GPU 기반 텐서 연산 및 딥러닝 학습 원리(역전파) 이해 | 선형 회귀 모델 수동 구현 및 그래디언트 계산 |
| **4일차** | **이론/실습:** 모델의 뼈대, nn.Module | **이론/실습:** 학습의 엔진, 옵티마이저와 데이터로더 | PyTorch를 이용한 신경망 구축 및 학습 파이프라인 완성 | nn.Module과 DataLoader를 활용한 학습 루프 구현 |
| **5일차** | **이론:** 이미지를 보는 눈, CNN | **실습:** CIFAR-10 이미지 분류기 제작 | CNN의 구조(합성곱, 풀링)와 작동 원리 이해 | torchvision과 nn.Conv2d를 사용한 CNN 모델링 |
| **6일차** | **이론:** 시간의 흐름을 읽다, RNN/LSTM | **실습:** 문자 단위 RNN으로 국적 분류하기 | 순차 데이터 처리와 RNN/LSTM의 장기 의존성 문제 해결 방식 이해 | 가변 길이 텍스트 데이터를 처리하는 RNN 모델링 |
| **7일차** | **이론:** 거인의 어깨 위에서, 트랜스포머 & 어텐션 | **실습:** Gemini API 첫걸음 | 트랜스포머와 셀프 어텐션 메커니즘의 개념적 이해 | Gemini API를 활용한 텍스트 생성 및 멀티모달 입력 처리 |
| **8일차** | **실습:** Python으로 웹 UI 만들기, Streamlit | **실습:** Gemini API 연동 대화형 챗봇 제작 | AI 모델을 서비스로 배포하기 위한 웹 애플리케이션 개발 능력 습득 | st.session\_state를 활용한 대화형 챗봇 구현 |
| **9일차** | **프로젝트:** 최종 프로젝트 기획 및 개발 스프린트 1 | **프로젝트:** 개발 스프린트 1 (계속) | 팀 협업을 통한 실전 AI 애플리케이션 기획 및 핵심 기능 개발 | 팀별 프로젝트 아키텍처 설계 및 역할 분담, 개발 착수 |
| **10일차** | **프로젝트:** 개발 스프린트 2 및 발표 준비 | **프로젝트:** 최종 발표 및 과정 수료 | 프로젝트 완성, 발표 및 동료/강사 피드백을 통한 종합 역량 강화 | 팀별 프로젝트 결과물 발표 및 시연, 과정 회고 |

---

## **일자별 상세 강의 계획**

### **1일차: AI의 초석 \- 데이터 과학을 위한 Python 마스터하기**

* **오전 (09:00-12:00) \- 이론: 인공지능의 큰 그림과 머신러닝 워크플로우**  
  * **목표:** 전체 AI 개발 프로세스 내 기술의 위치를 파악하여 학습 동기 부여 및 지식 체계화.  
  * **내용:**  
    * **AI, ML, DL 관계:** AI \> ML \> DL 포함 관계 정의 및 명확화.  
    * **학습 방식 분류:** 지도학습, 비지도학습, 강화학습의 개념 및 대표 예시 소개.  
    * **ML 프로젝트 생명주기:** 문제 정의부터 배포까지 이어지는 6단계 워크플로우 설명.  
* **오후 (13:00-18:00) \- 실습: 수치 연산의 핵심, NumPy**  
  * **목표:** 딥러닝의 기반이 되는 대규모 수치 데이터를 빠르고 효율적으로 다루는 NumPy 마스터.  
  * **내용:**  
    * **NumPy 필요성:** Python 리스트 대비 '벡터화(vectorized) 연산'을 통한 압도적 성능 제공. (PyTorch 텐서 연산의 기초)  
    * **핵심 기능 실습:**  
      * ndarray 생성(np.array, np.zeros 등) 및 조작(reshape).  
      * 인덱싱 및 슬라이싱.  
      * 벡터화 연산(+, \-, \*, /) 및 브로드캐스팅 규칙 이해.  
      * 불리언 마스킹을 이용한 조건부 데이터 추출.

### **2일차: 데이터 길들이기 \- Pandas와 Matplotlib**

* **오전 (09:00-12:00) \- 실습: 데이터 조작의 연금술, Pandas**  
  * **목표:** 정형 데이터의 로딩, 정제, 가공에 필수적인 Pandas 라이브러리 마스터.  
  * **내용:**  
    * **핵심 자료구조:** Series(1차원), DataFrame(2차원) 생성 및 기본 조작.  
    * **데이터 입출력:** pd.read\_csv, pd.read\_excel 등 파일 입출력.  
    * **데이터 정제:** 결측치 처리(isnull, dropna, fillna), 데이터 타입 변환(to\_datetime).  
    * **데이터 선택/가공:** loc/iloc 인덱서를 이용한 정밀 선택, 조건 필터링, groupby를 이용한 그룹화 및 집계(sum, mean, agg).  
* **오후 (13:00-18:00) \- 실습: 데이터에 생명 불어넣기, Matplotlib & Seaborn 시각화**  
  * **목표:** 데이터에 숨겨진 패턴과 인사이트를 발견하는 시각화 능력 배양.  
  * **내용:**  
    * **Matplotlib 기초:** 선 그래프(plot), 막대 그래프(bar), 산점도(scatter), 히스토그램(hist) 등 기본 그래프 작성 및 커스터마이징.  
    * **Seaborn 활용:** 더 적은 코드로 미려하고 통계적인 그래프 작성.  
    * **탐색적 데이터 분석(EDA):** 정제된 데이터를 바탕으로 변수 분포(히스토그램, 박스 플롯), 변수 간 관계(산점도, 히트맵)를 시각화하여 인사이트 도출.

### **3일차: 딥러닝의 문을 열다 \- PyTorch 기초**

* **오전 (09:00-12:00) \- 이론 및 실습: PyTorch의 심장, 텐서(Tensor)**  
  * **목표:** GPU 가속 연산이 가능한 PyTorch의 핵심 자료구조 텐서의 개념과 사용법 숙지.  
  * **내용:**  
    * **텐서 개념:** NumPy ndarray와 유사한 다차원 배열, GPU 연산 지원으로 대규모 병렬 계산에 특화.  
    * **텐서 생성/조작:** torch.tensor, torch.randn 등 생성 함수 및 인덱싱, 슬라이싱, view(reshape) 실습.  
    * **GPU 활용:** .to('cuda') 메소드를 이용, 텐서를 GPU로 이동시켜 연산 가속 체험.  
* **오후 (13:00-18:00) \- 이론 및 실습: 자동 미분의 마법, torch.autograd**  
  * **목표:** 딥러닝 학습의 핵심 원리인 역전파와 자동 미분 기능 autograd 이해.  
  * **내용:**  
    * **딥러닝 학습 원리:**  
      * **경사하강법:** 손실(loss)을 최소화하는 방향(기울기)으로 파라미터를 업데이트하는 과정.  
      * **역전파:** 출력층에서부터 각 파라미터의 기울기를 연쇄 법칙으로 효율적으로 계산하는 알고리즘.1  
    * **autograd 실습:**  
      * requires\_grad=True: 텐서의 연산을 추적하여 기울기 계산을 활성화.  
      * loss.backward(): 손실로부터 역전파를 자동 수행, .grad 속성에 기울기 저장.  
    * **실습 과제:** 선형 회귀 모델(y=w×x+b)을 수동으로 구현, loss.backward() 호출 후 w.grad와 b.grad를 이용해 직접 파라미터를 업데이트하며 손실 감소 과정 확인.

### **4일차: 진짜 신경망 만들기 \- 모델 구축과 학습**

* **오전 (09:00-12:00) \- 이론 및 실습: 모델의 뼈대, nn.Module과 손실 함수**  
  * **목표:** PyTorch의 고수준 API를 활용하여 재사용 가능한 신경망 모델 구축.  
  * **내용:**  
    * **torch.nn.Module:** 모든 신경망 모델의 부모 클래스.  
      * \_\_init\_\_(self): 모델에 필요한 계층(nn.Linear 등) 정의.  
      * forward(self, x): 입력 데이터의 연산 흐름 정의.  
    * **손실 함수:** nn.MSELoss(회귀), nn.CrossEntropyLoss(분류) 등 사전 구현된 손실 함수 사용법 학습.  
    * **실습 과제:** 3일차의 선형 회귀 모델을 nn.Module과 nn.Linear를 사용해 클래스 형태로 재작성.  
* **오후 (13:00-18:00) \- 이론 및 실습: 학습의 엔진, 옵티마이저와 데이터로더**  
  * **목표:** 학습 과정을 자동화하는 옵티마이저와 데이터로더의 역할 이해 및 활용.  
  * **내용:**  
    * **옵티마이저:** torch.optim 패키지(optim.SGD, optim.Adam)를 이용한 자동 파라미터 업데이트.  
    * **표준 학습 루프 (3단계):**  
      1. optimizer.zero\_grad(): 기울기 초기화.  
      2. loss.backward(): 역전파로 기울기 계산.  
      3. optimizer.step(): 파라미터 업데이트.  
    * **Dataset & DataLoader:** 대용량 데이터를 효율적으로 관리.  
      * Dataset: 데이터와 레이블을 하나씩 반환하는 로직 정의.  
      * DataLoader: Dataset을 받아 배치(batch) 단위로 묶고, 셔플링 등 기능 제공.  
    * **실습 과제:** Dataset, DataLoader, nn.Module 모델, Optimizer를 결합하여 완전한 학습 루프 구현.

### **5일차: 이미지를 보는 눈 \- CNN (합성곱 신경망)**

* **오전 (09:00-12:00) \- 이론: CNN의 시각적 직관**  
  * **목표:** 이미지 처리에 특화된 CNN의 구조와 작동 원리를 직관적으로 이해.  
  * **내용:**  
    * **합성곱(Convolution) 연산:** 커널(필터)이 이미지를 스캔하며 국소적 특징(선, 질감 등)을 추출, 특징 맵(Feature Map) 생성.2  
    * **주요 파라미터:** 채널, 패딩, 스트라이드 개념 이해.  
    * **풀링(Pooling) 레이어:** 특징 맵의 크기를 줄여(downsampling) 계산량을 감소시키고, 위치 변화에 덜 민감한 모델 생성.2  
    * **특징 계층:** 저수준 특징(선, 모서리)에서 고수준 특징(눈, 코, 입)으로 학습이 진행되는 계층적 구조 이해.  
* **오후 (13:00-18:00) \- 실습: CIFAR-10 이미지 분류기 제작**  
  * **목표:** PyTorch 지식을 총동원하여 실제 이미지 분류기 제작.  
  * **내용:**  
    * **데이터 준비:** torchvision을 이용한 CIFAR-10 데이터셋 다운로드, 정규화 등 전처리 및 DataLoader 생성.3  
    * **CNN 모델 정의:** nn.Module 내에 nn.Conv2d, nn.MaxPool2d 레이어를 조합하여 CNN 구조 설계.3  
    * **학습 및 평가:** nn.CrossEntropyLoss와 optim.SGD를 사용하여 모델을 학습시키고, 테스트 데이터셋으로 정확도 평가.3

### **6일차: 시간의 흐름을 읽다 \- RNN (순환 신경망)**

* **오전 (09:00-12:00) \- 이론: 순차 데이터와 RNN/LSTM의 원리**  
  * **목표:** 텍스트, 시계열 등 순차 데이터 처리 모델인 RNN과 LSTM의 원리 이해.  
  * **내용:**  
    * **RNN (Recurrent Neural Network):** 네트워크 내 순환 구조를 통해 이전 타임스텝의 정보(은닉 상태)를 현재 입력과 함께 처리.  
    * **장기 의존성 문제:** 시퀀스가 길어질수록 과거 정보가 소실되는 RNN의 한계.  
    * **LSTM (Long Short-Term Memory):** 장기 의존성 문제 해결을 위한 진보된 RNN 구조.  
      * **셀 상태(Cell State):** 정보가 장기간 보존될 수 있는 '메모리 컨베이어 벨트'.4  
      * **게이트(Gates):** Forget, Input, Output 3개의 게이트를 통해 셀 상태의 정보를 정교하게 제어.4  
* **오후 (13:00-18:00) \- 실습: 문자 단위(Character-level) RNN으로 이름 국적 분류하기**  
  * **목표:** 가변 길이의 순차 데이터를 처리하는 RNN 모델링 실습.  
  * **내용:**  
    * **데이터 전처리:** 텍스트(문자)를 고유 정수 인덱스 및 원-핫 벡터 텐서로 변환.5  
    * **RNN 모델 정의:** nn.Module 내에 nn.RNN 또는 nn.LSTM 레이어를 사용하여 문자 단위 RNN 모델 구현.5  
    * **학습 및 예측:** 모델 학습 후, 새로운 이름을 입력하여 국적을 예측하며 순차 패턴 학습 과정 확인.5

### **7일차: 거인의 어깨 위에서 \- LLM과 Gemini API**

* **오전 (09:00-12:00) \- 이론: 트랜스포머와 어텐션 메커니즘**  
  * **목표:** 현대 NLP의 근간인 트랜스포머 아키텍처의 핵심 원리 이해.  
  * **내용:**  
    * **트랜스포머 등장 배경:** RNN의 순차 처리 한계(병렬화 불가, 장기 의존성) 극복.  
    * **셀프 어텐션(Self-Attention):** 문장 내 모든 단어 간의 연관성을 동시에 계산하여 문맥을 파악하는 핵심 메커니즘. Query, Key, Value 벡터를 통해 작동.7  
    * **트랜스포머 구조:** 인코더-디코더 구조, 멀티헤드 어텐션을 통한 다각적 문맥 학습.7  
* **오후 (13:00-18:00) \- 실습: Gemini API 첫걸음**  
  * **목표:** 사전 훈련된 거대 모델(LLM)을 API로 활용하는 패러다임 전환 경험.  
  * **내용:**  
    * **API 키 발급 및 설정:** Google AI Studio에서 API 키 발급 및 환경 설정.8  
    * **Python SDK 사용:** pip install google-generativeai 설치 및 기본 설정.8  
    * **텍스트 생성:** model.generate\_content() 함수를 이용한 기본 텍스트 생성 및 프롬프트 엔지니어링 탐구.8  
    * **멀티모달 입력 처리:** 텍스트와 이미지를 동시에 입력으로 사용하여 이미지 설명, 질의응답 등 고차원 AI 기능 구현.9

### **8일차: AI를 서비스로 \- Streamlit 기반 애플리케이션 제작**

* **오전 (09:00-12:00) \- 실습: Python으로 웹 UI 만들기, Streamlit**  
  * **목표:** 복잡한 웹 기술 없이 Python만으로 AI 모델을 서비스할 수 있는 웹 애플리케이션 제작 기술 습득.  
  * **내용:**  
    * **Streamlit 철학:** 데이터 과학자를 위한 빠르고 쉬운 웹 앱 개발 프레임워크.  
    * **기본 위젯:** st.title, st.write, st.button, st.text\_input 등 UI 컴포넌트 사용법 학습.  
    * **레이아웃:** st.sidebar, st.columns를 이용한 화면 구성.  
    * **상태 관리:** st.session\_state를 이용해 상호작용 간 데이터(대화 기록 등) 유지.  
* **오후 (13:00-18:00) \- 실습: Gemini API와 Streamlit을 연동한 대화형 챗봇 만들기**  
  * **목표:** 학습한 기술을 총동원하여 실제 작동하는 대화형 챗봇 애플리케이션 제작.  
  * **내용:**  
    * **챗봇 UI 구성:** st.chat\_message와 st.chat\_input을 이용한 대화 인터페이스 구현.  
    * **대화 기록 관리:** st.session\_state에 대화 내용을 리스트로 저장 및 화면에 표시.  
    * **API 연동 로직:** 사용자 입력 시, 전체 대화 기록을 Gemini API에 전달하여 문맥에 맞는 답변을 받고, 이를 다시 st.session\_state에 저장 및 화면에 표시하는 로직 구현.9

### **9-10일차: 최종 관문 \- AI 애플리케이션 팀 프로젝트**

* **개요:** 개별 기술 학습을 넘어 '문제 정의, 기술 선택, 협업, 발표'의 소프트웨어 엔지니어링 전 과정 축소 경험.  
* **9일차 오전 \- 프로젝트 기획**  
  * **목표:** '작동하는 AI 서비스 프로토타입 제작'.  
  * **아이디어 브레인스토밍:** PyTorch, Gemini API, 멀티모달 등 학습 내용을 기반으로 자유롭게 아이디어 제안 및 토론.  
  * **팀 구성 및 아이디어 선정:** 관심사 기반 팀 구성 및 최종 프로젝트 아이디어 확정.  
* **9일차 오후 \- 개발 스프린트 1**  
  * **설계 및 역할 분담:** 시스템 아키텍처 설계 및 팀원 간 역할 분담.  
  * **핵심 기능 개발:** 역할에 따라 개발 착수, 강사 및 조교의 기술 멘토링 진행.  
* **10일차 오전 \- 개발 스프린트 2 및 발표 준비**  
  * **통합 및 디버깅:** 개발된 기능 통합 및 최종 테스트, 버그 수정.  
  * **발표 준비:** 프로젝트 목적, 사용 기술, 구현 과정, 시연 시나리오를 담은 발표 자료 준비.  
* **10일차 오후 \- 최종 발표 및 과정 수료**  
  * **최종 발표:** 팀별 프로젝트 결과물 발표 및 시연.  
  * **피드백 및 회고:** 동료 및 강사 피드백, 과정 전체 회고.  
  * **수료:** 과정 이수자 대상 수료증 수여 및 교육과정 마무리.
