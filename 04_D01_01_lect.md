---

# **1일차 오전: 인공지능의 큰 그림과 머신러닝 워크플로우**

---

## **1\. 인공지능(AI), 머신러닝(ML), 딥러닝(DL)의 관계와 역사**

\[필요한 배경지식\]  
인공지능, 머신러닝, 딥러닝, CPU, GPU, AI 가속기

### **1.1. AI, ML, DL의 개념**

* **인공지능 (Artificial Intelligence, AI):** 가장 넓은 개념입니다. 인간의 지능적인 행동(학습, 추론, 지각 등)을 모방하도록 만들어진 컴퓨터 시스템을 의미합니다. 단순히 정해진 규칙대로만 움직이는 것을 넘어, 스스로 생각하고 문제를 해결하는 것을 목표로 합니다.

  * **예시:** 영화에 나오는 사람처럼 대화하는 로봇, 복잡한 게임에서 인간 챔피언을 이기는 프로그램, 특정 사용자에게 적합한 상품을 추천하는 시스템, 자율주행 자동차 등이 모두 AI의 범주에 속합니다.

* **머신러닝 (Machine Learning, ML):** 인공지능을 구현하는 한 가지 접근 방식입니다. 개발자가 모든 규칙을 직접 프로그래밍하는 대신, 컴퓨터에 대량의 데이터를 제공하여 스스로 학습하게 만드는 방법입니다. 즉, 경험(데이터)을 통해 성능이 향상되는 컴퓨터 알고리즘이라 할 수 있습니다.

  * **핵심 아이디어:** "명시적으로 프로그래밍되지 않고도 컴퓨터가 학습할 수 있는 능력을 부여하는 연구 분야" \- 아서 사무엘(Arthur Samuel), 1959\.

  * **예시:** 수많은 고양이 사진과 강아지 사진을 보여주면, 컴퓨터가 스스로 둘을 구분하는 특징을 학습하여 새로운 사진을 분류하게 됩니다. 넷플릭스(Netflix)와 같은 스트리밍 서비스에서 사용자의 시청 기록을 분석하여 취향에 맞는 영화를 추천하는 시스템도 머신러닝의 대표적인 예입니다.

* **딥러닝 (Deep Learning, DL):** 머신러닝의 한 분야로, '인공 신경망(Artificial Neural Network)'이라는 방법을 사용합니다. 특히, 이 신경망을 매우 깊게(Deep) 쌓아서 복잡한 패턴을 학습하는 데 뛰어난 성능을 보입니다. 인간의 뇌가 뉴런들의 연결을 통해 학습하는 방식에서 영감을 얻었습니다.

  * **특징:** 이미지, 음성, 텍스트와 같이 비정형적이고 복잡한 데이터에서 강력한 성능을 발휘합니다. 우리가 앞으로 배울 PyTorch는 바로 이 딥러닝을 위한 도구입니다.

  * **예시:** 자율주행 자동차가 도로의 차선, 신호등, 다른 차량을 인식하는 기술, 스마트폰의 음성 비서가 사람의 말을 알아듣는 기술, 그리고 최근 열풍을 일으키고 있는 ChatGPT와 같은 대규모 언어 모델(LLM)을 활용한 텍스트 생성 및 번역 기술이 딥러닝의 대표적인 산물입니다.

\[그림, AI, ML, DL의 포함 관계를 보여주는 벤 다이어그램. 가장 큰 원이 AI, 그 안에 ML, 가장 안쪽에 DL이 있는 형태. 각 영역에 키워드(https://www.google.com/search?q=%EC%98%88: AI-지식표현,탐색 / ML-학습 / DL-신경망) 포함\]  
이미지 생성 프롬프트 제안: A simple, clean Venn diagram illustrating the relationship between Artificial Intelligence, Machine Learning, and Deep Learning. The outermost circle is labeled "Artificial Intelligence (AI)" and contains keywords like "Reasoning, Knowledge Representation". Inside it, a smaller circle is labeled "Machine Learning (ML)" with keywords like "Learning from Data, Algorithms". Inside the ML circle, the smallest circle is labeled "Deep Learning (DL)" with the keyword "Artificial Neural Networks".

### **1.2. 인공지능의 역사: 기대와 시련의 반복**

인공지능의 역사는 부흥기(AI Spring)와 침체기(AI Winter)를 반복하며 발전해 왔습니다.

* **태동기 (1950s \~ 1970s): AI의 탄생과 황금기**  
  * 1956년, 다트머스 회의(Dartmouth Workshop): '인공지능(Artificial Intelligence)'이라는 용어가 처음으로 등장하며 AI 연구가 공식적으로 시작되었습니다. 당시 과학자들은 1세대 안에 인간 수준의 지능을 가진 기계가 나올 것이라 예측할 정도로 매우 낙관적이었습니다.

* **1차 AI 겨울 (1970s 중반 \~ 1980s 초반): 환상과 한계**  
  * 너무 높았던 기대와 달리, 복잡한 현실 세계의 문제를 푸는 데 한계가 드러났습니다. 컴퓨터의 연산 능력 부족과 이론의 한계로 인해 연구 성과가 더뎌지자, 정부와 기업의 연구 지원이 대폭 삭감되는 'AI 겨울'이 찾아왔습니다.

* **부활기 (1980s): 전문가 시스템의 등장**  
  * 특정 분야의 전문가 지식을 컴퓨터에 저장해, 전문가처럼 문제를 해결하는 \*\*'전문가 시스템(Expert System)'\*\*이 상업적으로 성공하며 AI는 다시 주목받기 시작했습니다.

* **2차 AI 겨울 (1980s 후반 \~ 1990s): 다시 찾아온 침체기**  
  * 전문가 시스템은 지식을 업데이트하고 유지하는 데 비용이 많이 들고, 특정 분야를 벗어나면 쓸모가 없다는 한계에 부딪혔습니다. 이로 인해 다시 한번 AI에 대한 관심과 투자가 줄어드는 시기를 맞았습니다.

* **조용한 혁명과 딥러닝의 부상 (2000s \~ 현재): 진정한 도약**  
  * 2006년, 제프리 힌튼(Geoffrey Hinton) 교수팀이 '딥 빌리프 네트워크(DBN)'를 통해 심층 신경망의 학습 문제를 해결하는 방법을 제시하며 딥러닝의 부활을 알렸습니다.

  * **빅데이터의 폭발적인 증가:** 인터넷의 발전과 센서 기술의 확산으로 인해 이미지, 텍스트, 음성 등 대량의 데이터가 축적되기 시작했고, 이는 딥러닝 모델 학습에 필수적인 '연료'가 되었습니다.  
  * **알고리즘 개선:** 역전파(Backpropagation)와 같은 기존 알고리즘이 개선되고, ReLU(Rectified Linear Unit)와 같은 새로운 활성화 함수, 드롭아웃(Dropout)과 같은 정규화 기법 등이 개발되면서 심층 신경망의 학습이 더욱 효율적으로 이루어졌습니다.  
  * 2012년, 이미지넷(ImageNet) 대회: 딥러닝 기반의 모델 'AlexNet'이 압도적인 성능으로 우승하며 딥러닝의 잠재력을 전 세계에 증명했습니다.

  * 이후 GPU 등 하드웨어의 발전, 빅데이터의 등장, 알고리즘의 진화가 맞물리면서 AI는 폭발적으로 성장했고, 현재 우리는 \*\*생성형 AI(Generative AI)\*\*가 일상에 스며드는 AI의 대중화 시대를 살고 있습니다.

### **1.3. AI 발전의 엔진: 프로세서의 진화**

AI, 특히 딥러닝의 발전은 프로세서 기술의 발전과 궤를 같이합니다. AI 알고리즘이 요구하는 방대한 양의 계산을 처리할 수 있는 하드웨어가 있었기에 오늘날의 AI 혁명이 가능했습니다.

* **CPU (Central Processing Unit, 중앙 처리 장치):**  
  * 컴퓨터의 '두뇌'로, 복잡하고 순차적인 명령을 빠르게 처리하도록 설계되었습니다. 초기 AI 연구는 모두 CPU를 기반으로 이루어졌지만, 코어 수가 적은 CPU는 AI 모델의 복잡한 병렬 연산을 처리하는 데 한계를 보였습니다.

* **GPU (Graphics Processing Unit, 그래픽 처리 장치):**  
  * 원래는 3D 그래픽 처리를 위해 탄생했으며, 수천 개의 작은 코어를 이용해 단순한 계산을 동시에, 병렬적으로 처리하는 데 특화되어 있습니다. 딥러닝의 핵심인 행렬 곱셈이 바로 이런 병렬 계산에 해당합니다.

  * GPU의 등장은 딥러닝의 "빅뱅"을 일으켰습니다. CPU로 몇 주가 걸릴 학습을 GPU는 며칠 만에 끝낼 수 있게 되면서, 연구자들은 더 크고 복잡한 모델을 실험할 수 있게 되었습니다.

  * 특히, 이 부트캠프에서 사용하는 Google Colab이 무료로 제공하는 GPU 지원 덕분에 학습자들은 고가의 하드웨어 없이도 딥러닝 모델의 복잡한 연산을 효율적으로 경험할 수 있습니다.

* **AI 전용 프로세서 (NPU, TPU 등):**  
  * 딥러닝 연산에 더욱 특화된 프로세서입니다.

  * **NPU(Neural Processing Unit):** 인간의 신경망을 모방하여 AI 연산을 가속하도록 설계되었으며, 특히 모바일 기기 등 저전력 환경에서 효율적입니다.

  * **TPU(Tensor Processing Unit):** 구글이 개발한 AI 가속기로, 대규모 AI 모델의 학습 및 추론에 뛰어난 성능과 전력 효율을 보여줍니다.

\[그림, CPU, GPU, NPU의 코어 구조와 작업 처리 방식을 비교하는 이미지. CPU는 소수의 강력한 코어가 순차적으로 작업을 처리하는 모습, GPU는 수많은 작은 코어가 동시에 작업을 처리하는 모습, NPU는 신경망 연산에 특화된 회로가 데이터를 처리하는 모습으로 표현\]  
이미지 생성 프롬프트 제안: An infographic comparing the processing architectures of CPU, GPU, and NPU for AI tasks. For CPU, show a few large, powerful cores processing tasks sequentially (one after another). For GPU, show thousands of small cores processing many simple tasks in parallel (all at once). For NPU, show a specialized circuit diagram representing a neural network, highlighting its efficiency for AI-specific calculations.

\[요약 및 용어 정리\]  
요약: AI는 가장 큰 꿈(AI ⊃ ML ⊃ DL)이며, 그 역사는 기대와 실망(AI 겨울)을 반복했습니다\[cite: 309\]. 특히 딥러닝의 폭발적인 성장은 GPU의 병렬 처리 능력, 빅데이터의 증가, 그리고 알고리즘의 개선 덕분에 가능했으며, 현재는 NPU, TPU 등 더욱 전문화된 AI 프로세서가 발전을 이끌고 있습니다\[cite: 309\].  
용어:  
\* AI 겨울(AI Winter): AI 연구에 대한 관심과 자금 지원이 급격히 줄어드는 침체기\[cite: 311\].  
\* CPU(Central Processing Unit): 소수의 강력한 코어로 순차적 작업에 능한 범용 프로세서\[cite: 312\].  
\* GPU(Graphics Processing Unit): 수많은 코어로 병렬 연산에 특화되어 딥러닝을 가속화한 프로세서\[cite: 313\].  
\* NPU/TPU (AI Accelerator): AI 연산에만 특화되어 효율을 극대화한 전용 프로세서\[cite: 314\].

---

## **2\. 머신러닝의 학습 방식 분류**

\[필요한 배경지식\]  
지도학습, 비지도학습, 강화학습, 선형 회귀, 로지스틱 회귀, 레이블(정답), 보상, 시그모이드 함수, 이진 교차 엔트로피

머신러닝은 '어떻게 학습하는가', 즉 데이터에 정답이 주어지는지 여부에 따라 크게 세 가지 방식으로 나눌 수 있습니다. 본격적으로 방식들을 알아보기 전에, 지도학습의 가장 기본이 되는 두 가지 알고리즘인

**선형 회귀**와 **로지스틱 회귀**를 통해 머신러닝이 데이터를 어떻게 학습하고 예측하는지 먼저 살펴보겠습니다.

### **2.1. 머신러닝 맛보기: 기본 모델 이해하기**

#### **선형 회귀 (Linear Regression) & 최적화**

선형 회귀는 여러 데이터들 사이의 관계를 가장 잘 나타내는

**직선**을 찾는 알고리즘입니다. 공부 시간과 시험 점수처럼, 하나의 변수(x)가 다른 변수(y)에 영향을 줄 때, "공부 시간이 늘어나면 시험 점수도 대체로 높아진다"와 같은 선형적인 관계를 모델링합니다.

* **목표:** 연속적인 값을 예측하는 것 (e.g., 집 크기에 따른 가격 예측).

* **가설 (Hypothesis):** H(x)=Wx+b

  * x: 입력 데이터 (https://www.google.com/search?q=%EC%98%88: 공부 시간) 

  * W: 가중치(Weight), 직선의 기울기. x가 결과에 얼마나 큰 영향을 미치는지 나타냅니다.

  * b: 편향(bias), 직선의 y절편. 기본적인 시작점을 나타냅니다.

* **학습:** 머신러닝 모델은 수많은 데이터(x,y 쌍)를 보고, 실제 정답(y)과 모델의 예측(H(x)) 사이의 평균적인 오차(비용 함수, Cost Function)를 최소화하는 최적의 W와 b를 찾아냅니다.

* **핵심 질문:** 수많은 직선 중에 어떤 직선이 데이터를 가장 잘 대표하는 '최적의 직선'일까요? 모델은 어떻게 최적의 W와 b를 찾을까요?

#### **모델 최적화: 경사 하강법으로 최적의 직선 찾기**

모델이 학습한다는 것은

**손실(Loss) 또는 비용(Cost)을 최소화하는 과정**입니다. 즉, 모델의 예측이 얼마나 틀렸는지를 나타내는 지표를 만들고, 이 값을 가장 작게 만드는

W와 b를 찾아내는 것입니다.

1. **손실 함수 (Loss Function) 정의: "모델이 얼마나 틀렸는가?"**  
   * 가장 대표적인 손실 함수는 \*\*평균 제곱 오차(Mean Squared Error, MSE)\*\*입니다.

   * 이는 각 데이터에 대해  
     **실제 정답(y\_i)과 모델의 예측값(H(x\_i))의 차이를 제곱**하고, 그 값들을 모두 더해 평균을 낸 것입니다.

   * Cost(W,b)=n1​i=1∑n​(H(xi​)−yi​)2=n1​i=1∑n​(Wxi​+b−yi​)2

   * 오차를 제곱하기 때문에, 예측이 정답보다 크든 작든 모두 양수로 만들어주고, 오차가 클수록 더 큰 벌점(패널티)을 주게 됩니다.

   * 우리의 목표는 이 $Cost(W, b)$를 최소로 만드는  
     W와 b를 찾는 것입니다.

2. **경사 하강법 (Gradient Descent) 적용: "손실을 줄이는 방향으로 이동"**  
   * 경사 하강법은 손실 함수의 값을 가장 빠르고 낮게 줄일 수 있는 방향을 찾아, 그 방향으로  
     W와 b를 조금씩 이동시키는 반복적인 최적화 방법입니다. 마치 안개 속에서 산을 내려올 때, 현재 위치에서 가장 가파른 경사(기울기)를 따라 한 걸음씩 내려가는 것과 같습니다.

   * **핵심 요소:**  
     * **기울기(Gradient):** 손실 함수를 W와 b에 대해 각각 미분(편미분)한 값. 현재 위치에서 손실이 가장 크게 증가하는 방향을 알려줍니다. 우리는 이

       **반대 방향**으로 가야 손실이 줄어듭니다.

     * **학습률(Learning Rate, alpha):** 기울기 방향으로 얼마나 큰 보폭으로 이동할지를 결정하는 값입니다. 너무 크면 최저점을 지나쳐 버리고, 너무 작으면 학습이 매우 오래 걸립니다. 적절한 학습률을 찾는 것이 중요합니다.

     * **에포크(Epoch):** 전체 학습 데이터셋을 한 번 모두 사용해 학습을 완료한 횟수입니다. 1 에포크는 전체 데이터를 1회 학습했음을 의미합니다.

   * **파라미터 업데이트 규칙:**  
     * WleftarrowW−alphatimes(text손실함수를W로편미분한값) 

     * bleftarrowb−alphatimes(text손실함수를b로편미분한값) 

Python

\# \[맛보기 실습 코드: 경사 하강법으로 선형 회귀 직접 구현하기\]  
import numpy as np  
import matplotlib.pyplot as plt

\# 1\. 데이터 준비 (공부 시간과 시험 점수)  
X \= np.array(\[1, 2, 3, 4, 5, 6, 7, 8\]) \# 입력 데이터 (독립 변수)  
y \= np.array(\[12, 18, 33, 41, 55, 62, 75, 78\]) \# 정답 데이터 (종속 변수)

\# 2\. 파라미터 초기화 및 하이퍼파라미터 설정  
W \= np.random.rand() \# 가중치(기울기) 랜덤 초기화  
b \= np.random.rand() \# 편향(y절편) 랜덤 초기화  
learning\_rate \= 0.01 \# 학습률  
epochs \= 2001        \# 에포크(학습 횟수)  
n \= len(X)           \# 데이터 개수  
loss\_history \= \[\]    \# 손실 기록을 위한 리스트

\# 3\. 경사 하강법 기반 학습 루프  
for i in range(epochs):  
    \# (1) 현재 W, b를 기반으로 예측값 계산 (Hypothesis)  
    hypothesis \= W \* X \+ b

    \# (2) 손실(Loss) 계산 (MSE)  
    cost \= np.sum((hypothesis \- y) \*\* 2) / n  
    loss\_history.append(cost)

    \# (3) 기울기(Gradient) 계산  
    gradient\_w \= np.sum((hypothesis \- y) \* 2 \* X) / n  
    gradient\_b \= np.sum((hypothesis \- y) \* 2) / n

    \# (4) 파라미터 업데이트  
    W \= W \- learning\_rate \* gradient\_w  
    b \= b \- learning\_rate \* gradient\_b

    if i % 200 \== 0:  
        print(f"Epoch {i:4d} | Cost: {cost:.2f} | W: {W:.2f} | b: {b:.2f}")

\# 4\. 결과 시각화  
plt.figure(figsize=(12, 5))

\# Plot 1: 학습 결과 시각화  
plt.subplot(1, 2, 1)  
plt.scatter(X, y, label='Actual Data')  
plt.plot(X, W \* X \+ b, color='red', label='Optimized Regression Line')  
plt.title('Linear Regression with Gradient Descent')  
plt.xlabel('Study Hours')  
plt.ylabel('Exam Scores')  
plt.legend()  
plt.grid(True)

\# Plot 2: 손실(Loss) 감소 과정 시각화  
plt.subplot(1, 2, 2)  
plt.plot(loss\_history)  
plt.title('Loss Reduction Over Epochs')  
plt.xlabel('Epoch')  
plt.ylabel('MSE Loss')  
plt.grid(True)  
plt.tight\_layout()  
plt.show()

print("\\n\[최종 학습 결과\]")  
print(f"Optimized W: {W:.2f}")  
print(f"Optimized b: {b:.2f}")

\[그림, 위 코드를 실행했을 때 나오는 두 개의 그래프. 왼쪽 그래프는 실제 데이터 점들과 함께 학습된 최적의 회귀 직선을 보여주고, 오른쪽 그래프는 에포크가 진행됨에 따라 손실(Loss) 값이 급격히 감소하다가 점차 수렴하는 과정을 보여주는 꺾은선 그래프\]  
이미지 생성 프롬프트 제안: A figure with two subplots side-by-side. The left subplot is a scatter plot of data points with a final regression line fitted through them, titled "Linear Regression with Gradient Descent". The right subplot is a line chart showing the value of the loss function decreasing over time (epochs), titled "Loss Reduction Over Epochs". The y-axis is labeled "MSE Loss" and the x-axis is "Epoch".

#### **로지스틱 회귀 (Logistic Regression)**

로지스틱 회귀는 '회귀'라는 이름 때문에 혼동하기 쉽지만, 실제로는 \*\*분류(Classification)\*\*를 위한 알고리즘입니다. 두 개의 선택지 중 하나를 결정하는 문제(https://www.google.com/search?q=%EC%98%88: '합격' 또는 '불합격', '스팸' 또는 '정상')에 주로 사용됩니다.

* **목표:** 데이터가 특정 범주에 속할 확률을 0과 1 사이의 값으로 예측하는 것.

* **핵심 아이디어:** **시그모이드 함수 (Sigmoid Function)**  선형 회귀의 결과(

  Wx+b)는 음의 무한대에서 양의 무한대까지 모든 값을 가질 수 있습니다. 이를 확률값(0\~1)으로 바꾸기 위해 시그모이드 함수를 사용합니다.  
  S(z)=frac11+e−z (z=Wx+b). 이 함수의 결과가 0.5보다 크면 1(https://www.google.com/search?q=%EC%98%88: 합격), 0.5보다 작으면 0(https://www.google.com/search?q=%EC%98%88: 불합격)으로 분류합니다.

* **핵심 질문:** 모델은 어떻게 '합격' 또는 '불합격'을 판단하는 최적의 \*\*결정 경계(Decision Boundary)\*\*를 학습할까요? 

1. **시그모이드 함수 (Sigmoid Function)**  
   * 선형 회귀의 예측값(  
     Wx+b)은 직선이므로 음의 무한대에서 양의 무한대까지 뻗어 나갑니다. 이를 0과 1 사이의

     **확률** 값으로 바꾸기 위해 **시그모이드 함수**를 사용합니다.

   * **정의:** 시그모이드 함수는 어떤 실수 입력(z)이든 받아서 항상 0과 1 사이의 S자 형태 곡선으로 된 값을 출력합니다.  
     H(x)=textsigmoid(z)=frac11+e−z (여기서 z=Wx+b) 

   * **역할:** 이 함수를 통과한 예측값 $H(x)$는 이제 "입력 x가 주어졌을 때, 정답이 1일 확률"을 의미하게 됩니다. 예를 들어

     H(x)=0.8이라면, "합격할 확률이 80%"라고 해석할 수 있습니다. 일반적으로 이 확률이 0.5 이상이면 1(합격), 미만이면 0(불합격)으로 최종 분류합니다.

2. **손실 함수: 이진 교차 엔트로피 (Binary Cross-Entropy)**  
   * 분류 문제에서는 MSE보다  
     **이진 교차 엔트로피(Binary Cross-Entropy, BCE)** 손실 함수가 더 적합합니다. BCE는 모델이 예측한 확률과 실제 정답(0 또는 1\) 간의 차이를 측정합니다.

   * 정의:

     Cost(W,b)=−n1​i=1∑n​\[yi​log(H(xi​))+(1−yi​)log(1−H(xi​))\]

   * **작동 원리:**  
     * 실제 정답 y\_i=1일 때:  
       * 두 번째 항 $(1-y\_i)\\log(1-H(x\_i))$은 0이 되어 사라집니다.

       * 손실은 $-\\log(H(x\_i))$가 됩니다. 모델이 1에 가깝게 예측할수록(

         H(x\_i)to1), $-\\log(H(x\_i))$는 0에 가까워져 손실이 작아집니다. 반면 0에 가깝게 잘못 예측하면(

         H(x\_i)to0), 손실은 무한대로 커집니다.

     * 실제 정답 y\_i=0일 때:  
       * 첫 번째 항 $y\_i\\log(H(x\_i))$이 0이 되어 사라집니다.

       * 손실은 $-\\log(1-H(x\_i))$가 됩니다. 모델이 0에 가깝게 예측할수록(

         H(x\_i)to0), 손실이 작아집니다. 반면 1에 가깝게 잘못 예측하면(

         H(x\_i)to1), 손실은 무한대로 커집니다.

   * **결론적으로,** BCE는 모델이 정답을 맞추면 손실을 적게 주고, 틀린 답을 높은 확신으로 예측하면 엄청난 벌점(손실)을 주는 합리적인 함수입니다.

3. **경사 하강법 적용**  
   * 선형 회귀와 마찬가지로, 로지스틱 회귀도 BCE 손실 함수를 최소화하기 위해 경사 하강법을 사용합니다. 손실 함수를

     W와 b에 대해 각각 미분하여 기울기를 구하고, 학습률에 맞춰 파라미터를 업데이트하는 과정을 반복합니다.

Python

\# \[맛보기 실습 코드: 경사 하강법으로 로지스틱 회귀 직접 구현하기\]  
import numpy as np  
import matplotlib.pyplot as plt

\# 1\. 데이터 준비 (공부 시간과 합격 여부)  
X \= np.array(\[1, 2, 3, 4, 5, 6, 7, 8, 9, 10\]) \# 입력 데이터 (독립 변수)  
y \= np.array(\[0, 0, 0, 0, 1, 1, 1, 1, 1, 1\]) \# 정답 데이터 (0:불합격, 1:합격)

\# 2\. 시그모이드 함수 및 손실 함수 정의  
def sigmoid(z):  
    return 1 / (1 \+ np.exp(-z))

def binary\_cross\_entropy\_loss(y, hypothesis):  
    \# log(0)이 되는 것을 방지하기 위해 아주 작은 값(1e-9)을 더함  
    return \-np.mean(y \* np.log(hypothesis \+ 1e-9) \+ (1 \- y) \* np.log(1 \- hypothesis \+ 1e-9))

\# 3\. 파라미터 초기화 및 하이퍼파라미터 설정  
W \= np.random.rand() \# 가중치 랜덤 초기화  
b \= np.random.rand() \# 편향 랜덤 초기화  
learning\_rate \= 0.1  \# 학습률  
epochs \= 5001        \# 에포크  
n \= len(X)           \# 데이터 개수  
loss\_history \= \[\]    \# 손실 기록

\# 4\. 경사 하강법 기반 학습 루프  
for i in range(epochs):  
    \# (1) 예측값 계산 (Hypothesis)  
    z \= W \* X \+ b  
    hypothesis \= sigmoid(z)

    \# (2) 손실(Loss) 계산 (BCE)  
    cost \= binary\_cross\_entropy\_loss(y, hypothesis)  
    loss\_history.append(cost)

    \# (3) 기울기(Gradient) 계산  
    \# BCE 손실함수를 미분하면 MSE와 형태가 같아짐  
    gradient\_w \= np.sum((hypothesis \- y) \* X) / n  
    gradient\_b \= np.sum(hypothesis \- y) / n

    \# (4) 파라미터 업데이트  
    W \= W \- learning\_rate \* gradient\_w  
    b \= b \- learning\_rate \* gradient\_b

    if i % 500 \== 0:  
        print(f"Epoch {i:4d} | Cost: {cost:.4f} | W: {W:.2f} | b: {b:.2f}")

\# 5\. 결과 시각화  
plt.figure(figsize=(12, 5))

\# Plot 1: 학습 결과 시각화  
plt.subplot(1, 2, 1)  
plt.scatter(X, y, c=y, cmap='bwr', label='Actual Data (0:Fail, 1:Pass)')  
x\_range \= np.linspace(X.min() \- 1, X.max() \+ 1, 300)  
y\_range \= sigmoid(W \* x\_range \+ b)  
plt.plot(x\_range, y\_range, color='green', label='Logistic Regression Curve')  
plt.axhline(y=0.5, color='red', linestyle='--', label='Threshold (0.5)')  
plt.title('Logistic Regression with Gradient Descent')  
plt.xlabel('Study Hours')  
plt.ylabel('Probability of Passing')  
plt.legend()  
plt.grid(True)

\# Plot 2: 손실(Loss) 감소 과정 시각화  
plt.subplot(1, 2, 2)  
plt.plot(loss\_history)  
plt.title('Loss Reduction Over Epochs')  
plt.xlabel('Epoch')  
plt.ylabel('BCE Loss')  
plt.grid(True)  
plt.tight\_layout()  
plt.show()

### **2.2. 머신러닝 학습 방식의 종류**

이제 머신러닝의 학습 방식을 본격적으로 살펴보겠습니다94.

* **지도 학습 (Supervised Learning)**  
  * '정답(Label)'이 있는 데이터로 학습하는 방식입니다. 위에서 살펴본 선형 회귀와 로지스틱 회귀가 바로 지도 학습의 대표적인 예입니다.

  * **입력:** 문제(Input) \+ 정답(Label) 

  * **목표:** 새로운 문제가 주어졌을 때, 정답을 정확히 예측하기 

  * **대표 예시:**  
    * **분류(Classification):** 주어진 데이터가 어떤 그룹에 속하는지 맞추는 문제. (https://www.google.com/search?q=%EC%98%88: 이메일이 '스팸'인지 '정상'인지 분류하기) 

    * **회귀(Regression):** 연속적인 값을 예측하는 문제. (https://www.google.com/search?q=%EC%98%88: 집의 크기, 위치 정보로 '집값' 예측하기) 

* **비지도 학습 (Unsupervised Learning)**  
  * '정답(Label)'이 없는 데이터로 학습하는 방식입니다. 정답지는 없지만, 데이터 자체에 숨겨진 구조, 패턴, 특징을 스스로 찾아내도록 합니다.

  * **입력:** 문제(Input)만 존재 

  * **목표:** 데이터의 숨겨진 구조나 패턴 발견하기 

  * **대표 예시:**  
    * **군집화(Clustering):** 비슷한 특성을 가진 데이터들을 그룹으로 묶는 것. (https://www.google.com/search?q=%EC%98%88: 고객들의 구매 패턴을 분석하여 비슷한 성향의 '고객 그룹'으로 나누기) 

    * **차원 축소(Dimensionality Reduction):** 데이터의 중요한 특징은 유지하면서 데이터의 양(차원)을 줄이는 것.

* **강화 학습 (Reinforcement Learning)**  
  * '보상(Reward)'을 통해 학습하는 방식입니다. 지도학습처럼 정답이 정해져 있진 않지만, 어떤 행동을 했을 때 '잘했다(보상)' 또는 '잘못했다(벌점)'라는 피드백을 받으며 학습합니다.

  * **입력:** 현재 상태(State)와 보상(Reward) 

  * **목표:** 누적 보상을 최대화하는 최적의 행동 정책(Policy) 찾기 

  * **대표 예시:**  
    * **게임 AI:** 바둑, 체스 등에서 수많은 대국을 스스로 두어보며 이기는 전략을 학습 (https://www.google.com/search?q=%EC%98%88: 알파고) 

    * **로봇 제어:** 넘어지지 않고 걷는 법을 스스로 배우는 로봇 

\[요약 및 용어 정리\]  
요약:  
\* 선형 회귀는 데이터를 가장 잘 설명하는 '직선'을 찾아 연속된 값을 예측하기 위해 손실 함수(MSE)를 정의하고, 경사 하강법을 이용해 손실을 최소화하는 W와 b를 학습합니다\[cite: 322, 335, 340, 432, 435\].  
\* 로지스틱 회귀는 'S자 곡선'으로 0과 1 사이의 확률을 구해 분류 문제를 풀기 위해 시그모이드 함수를 사용하고, 이진 교차 엔트로피(BCE) 손실을 최소화하는 방향으로 경사 하강법을 이용해 최적의 분류 경계를 학습합니다\[cite: 364, 370, 377, 390, 438\].  
\* 머신러닝 학습 방식은 크게 정답지로 공부하는 지도학습, 데이터만 보고 패턴을 찾는 비지도학습, 상과 벌로 행동을 배우는 강화학습으로 나뉩니다\[cite: 401, 409, 416, 424\].

용어:  
\* 선형 회귀(Linear Regression): 데이터의 관계를 직선으로 모델링하여 연속적인 값을 예측하는 지도학습 알고리즘\[cite: 426\].  
\* 로지스틱 회귀(Logistic Regression): 시그모이드 함수를 사용해 확률을 예측하고, 이를 기반으로 이진 분류를 수행하는 지도학습 알고리즘\[cite: 427\].  
\* 손실 함수(Loss Function): 모델의 예측이 실제 정답과 얼마나 다른지를 나타내는 함수 (예: MSE, BCE)\[cite: 434\].  
\* 경사 하강법(Gradient Descent): 손실 함수의 기울기를 이용해 반복적으로 파라미터를 업데이트하며 손실을 최소화하는 최적화 알고리즘\[cite: 435\].  
\* 학습률(Learning Rate): 경사 하강법에서 파라미터를 업데이트할 때의 보폭(step size)\[cite: 436\].  
\* 에포크(Epoch): 전체 학습 데이터셋이 학습에 한 번 사용된 횟수\[cite: 437\].  
\* 시그모이드 함수(Sigmoid Function): 어떤 실수 입력이든 0과 1 사이의 S자 곡선 값으로 변환하는 함수. 확률을 모델링하는 데 사용됩니다\[cite: 441\].  
\* 이진 교차 엔트로피(BCE): 이진 분류 문제에서 모델의 예측 확률과 실제 정답 간의 차이를 측정하는 손실 함수\[cite: 442\].  
\* 결정 경계(Decision Boundary): 모델이 두 클래스를 구분하는 기준선. 로지스틱 회귀에서는 일반적으로 확률이 0.5가 되는 지점입니다\[cite: 443\].  
\* 지도학습(Supervised Learning): 레이블(정답)이 있는 데이터를 사용해 학습하는 방식\[cite: 428\].  
\* 비지도학습(Unsupervised Learning): 레이블 없이 데이터 자체의 구조를 학습하는 방식\[cite: 429\].  
\* 강화학습(Reinforcement Learning): 보상을 통해 시행착오를 겪으며 최적의 행동을 학습하는 방식\[cite: 430\].

---

## **3\. 머신러닝 프로젝트 워크플로우 (생명주기)**

\[필요한 배경지식\]  
프로젝트 생명주기, 데이터 수집, 데이터 전처리, 모델링, 평가, 배포

하나의 머신러닝 모델이 아이디어에서 시작해 실제 서비스로 탄생하기까지는 정해진 과정이 있습니다. 이 전체 흐름을 '머신러닝 프로젝트 생명주기' 또는 '워크플로우'라고 부릅니다. 우리 부트캠프는 이 모든 단계를 경험하는 것을 목표로 합니다.

\[그림, 머신러닝 워크플로우 6단계를 보여주는 순서도(Flowchart). 각 단계가 화살표로 연결되어 순환적인 프로세스임을 암시. (6)배포 단계에서 다시 (1)문제 정의로 돌아가는 화살표 추가 가능\]  
이미지 생성 프롬프트 제안: A flowchart illustrating the 6 steps of the Machine Learning Workflow. The steps are: 1\. Problem Definition, 2\. Data Collection & Preprocessing, 3\. Modeling, 4\. Training, 5\. Evaluation, 6\. Deployment & Monitoring. Each step is in a box and connected by arrows in a cycle. An arrow can loop from step 6 back to step 1 to show it's an iterative process.

### **단계별 상세 설명**

1. **문제 정의 (Problem Definition)** 

   * 가장 중요한 첫 단계입니다. 우리가 AI로 무엇을 해결하고 싶은지 명확하게 정의합니다.

   * 비즈니스 목표는 무엇이며, 성공은 어떻게 측정할 것인가? 

2. **데이터 수집 및 전처리 (Data Collection & Preprocessing)** 

   * 정의된 문제를 풀기 위해 필요한 데이터를 모으고, 모델이 학습할 수 있는 형태로 깨끗하게 다듬는 과정입니다.

   * 결측치(빈 값)를 채우거나, 데이터의 형식을 통일하는 등의 작업이 포함됩니다. 종종 프로젝트에서 가장 많은 시간이 소요되는 단계입니다.

   * (오늘 오후와 내일 배울 NumPy, Pandas가 여기서 활약합니다.) 

3. **모델링 (Modeling)** 

   * 문제에 가장 적합한 머신러닝 알고리즘(모델)을 선택하는 단계입니다.

   * 이미지 문제에는 CNN, 순차적인 텍스트 문제에는 RNN 등 데이터와 문제의 특성에 맞는 모델을 고릅니다. (5, 6일차에 배울 내용입니다.) 

4. **학습 (Training)** 

   * 준비된 데이터를 모델에 입력하여, 모델이 데이터 속 패턴을 학습하도록 하는 과정입니다.

   * 이 과정에서 모델 내부의 파라미터(Parameter)들이 최적의 값으로 조정됩니다. (3, 4일차에 PyTorch로 직접 구현해 볼 것입니다.) 

5. **평가 (Evaluation)** 

   * 학습이 완료된 모델이 얼마나 좋은 성능을 내는지 객관적인 지표로 평가합니다.

   * 학습에 사용되지 않은 새로운 데이터(테스트 데이터)를 이용해 모델의 일반화 성능을 확인합니다. (https://www.google.com/search?q=%EC%98%88: 정확도, 정밀도 등) 

6. **배포 및 모니터링 (Deployment & Monitoring)** 

   * 성능이 검증된 모델을 실제 사용자들이 쓸 수 있는 서비스(웹, 앱 등)에 탑재하는 마지막 단계입니다.

   * 배포 후에도 모델의 성능이 잘 유지되는지 지속적으로 관찰하고, 필요하다면 새로운 데이터로 모델을 업데이트합니다. (8일차 Streamlit과 9\~10일차 최종 프로젝트에서 경험하게 됩니다.) 

\[요약 및 용어 정리\]  
요약: 성공적인 AI 프로젝트는 (1) 명확한 문제 정의에서 시작해, (2) 깨끗한 데이터를 준비하고, (3) 적절한 모델을 골라 (4) 학습 시킨 후, (5) 냉정하게 평가하여 (6) 실제 서비스로 배포하는 6단계의 순환 과정을 거칩니다\[cite: 476\].  
용어:  
\* 워크플로우(Workflow): 작업 절차의 흐름\[cite: 478\].  
\* 전처리(Preprocessing): 원본 데이터를 모델 학습에 적합한 형태로 가공하는 과정\[cite: 479\].  
\* 모델링(Modeling): 문제 해결에 적합한 알고리즘을 선택하고 설계하는 과정\[cite: 480\].  
\* 배포(Deployment): 개발된 모델을 실제 서비스 환경에서 사용 가능하게 만드는 것\[cite: 481\].

---

### **오전 강의 마무리**

여러분, 수고 많으셨습니다. 오늘 오전에는 우리가 앞으로 10일간 항해할 'AI 개발'이라는 바다의 전체 지도를 그려보았습니다. AI, ML, DL의 관계부터 머신러닝의 학습 방식, 그리고 하나의 AI 서비스가 탄생하는 과정까지 살펴보았습니다.

이제 이 모든 과정의 가장 근본이 되는 '데이터'를 다루는 기술을 배울 시간입니다. 딥러닝은 본질적으로 거대한 숫자 계산의 연속이며, 오후에는 이 숫자들을 효율적으로 다루는 강력한 도구인

**NumPy**에 대해 실습과 함께 깊이 있게 알아보겠습니다.

